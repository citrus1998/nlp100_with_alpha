{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJ9L3HZdQY4WRyRTpY0m/f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 第6章: 機械学習  \n",
        "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．  "
      ],
      "metadata": {
        "id": "rTcwZ0jMgtyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5clo6J5DdBvM",
        "outputId": "acab114f-f8e2-4f89-aae3-a2eea25b732d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re,sys,os\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as scp\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn import feature_extraction, preprocessing\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, f1_score\n",
        "from sklearn.metrics import accuracy_score,  precision_score, recall_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "# import dask.array as da\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Neural Language Processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "import MeCab\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "Z4O7C_SIg18O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50. データの入手・整形\n",
        "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．  \n",
        "\n",
        "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．  \n",
        "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．  \n",
        "3. 抽出された事例をランダムに並び替える．  \n",
        "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．  \n",
        "5. 学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yd0u2TsIfVR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip --no-check-certificate\n",
        "!unzip ./NewsAggregatorDataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2fwMiQOfiO4",
        "outputId": "394bee62-47cd-4a67-f708-b063491626df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 03:36:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas     [    <=>             ]  27.87M  35.0MB/s    in 0.8s    \n",
            "\n",
            "2024-05-03 03:36:37 (35.0 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203]\n",
            "\n",
            "Archive:  ./NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
        "data0 = pd.read_csv('newsCorpora.csv',sep='\\t',header=None,\n",
        "                 names=[\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"])\n",
        "data = data0[data0['PUBLISHER'].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"])]"
      ],
      "metadata": {
        "id": "oOwR9xBShKb-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 抽出された事例をランダムに並び替える．\n",
        "# 4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
        "train0, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
        "train, val = train_test_split(train0, test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "TMm90tHGiIqg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
        "train.to_csv(\"train.txt\", sep=\"\\t\", index=False, header=None)\n",
        "val.to_csv(\"valid.txt\", sep=\"\\t\", index=False, header=None)\n",
        "test.to_csv(\"test.txt\", sep=\"\\t\", index=False, header=None)"
      ],
      "metadata": {
        "id": "r4_66AdvizO_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．\n",
        "print(\"train:{}\\nvalid:{}\\ntest:{}\".format(train.shape, val.shape, test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrtRRQXEk7_b",
        "outputId": "3ce90629-2c20-4b8a-f022-b4dea2b77bbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:(10805, 8)\n",
            "valid:(1201, 8)\n",
            "test:(1334, 8)\n"
          ]
        }
      ]
    }
  ]
}