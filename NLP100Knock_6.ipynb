{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtsMXhENC+yzNCARufzRaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s528661/nlp100/blob/JDLA_Engeneer_20240503/NLP100Knock_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第6章: 機械学習  \n",
        "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．  "
      ],
      "metadata": {
        "id": "rTcwZ0jMgtyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5clo6J5DdBvM",
        "outputId": "acab114f-f8e2-4f89-aae3-a2eea25b732d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install mecab-python3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re,sys,os\n",
        "\n",
        "import pickle\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as scp\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn import feature_extraction, preprocessing\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, f1_score\n",
        "from sklearn.metrics import accuracy_score,  precision_score, recall_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "# import dask.array as da\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Neural Language Processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "import MeCab\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "Z4O7C_SIg18O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50. データの入手・整形\n",
        "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．  \n",
        "\n",
        "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．  \n",
        "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．  \n",
        "3. 抽出された事例をランダムに並び替える．  \n",
        "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．  \n",
        "5. 学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yd0u2TsIfVR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip --no-check-certificate\n",
        "!unzip ./NewsAggregatorDataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2fwMiQOfiO4",
        "outputId": "394bee62-47cd-4a67-f708-b063491626df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 03:36:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas     [    <=>             ]  27.87M  35.0MB/s    in 0.8s    \n",
            "\n",
            "2024-05-03 03:36:37 (35.0 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203]\n",
            "\n",
            "Archive:  ./NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
        "data0 = pd.read_csv('newsCorpora.csv',sep='\\t',header=None,\n",
        "                 names=[\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"])\n",
        "data = data0[data0['PUBLISHER'].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"])]"
      ],
      "metadata": {
        "id": "oOwR9xBShKb-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 抽出された事例をランダムに並び替える．\n",
        "# 4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
        "train0, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
        "train, val = train_test_split(train0, test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "TMm90tHGiIqg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
        "train.to_csv(\"train.txt\", sep=\"\\t\", index=False, header=None)\n",
        "val.to_csv(\"valid.txt\", sep=\"\\t\", index=False, header=None)\n",
        "test.to_csv(\"test.txt\", sep=\"\\t\", index=False, header=None)"
      ],
      "metadata": {
        "id": "r4_66AdvizO_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．\n",
        "print(\"train:{}\\nvalid:{}\\ntest:{}\".format(train.shape, val.shape, test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrtRRQXEk7_b",
        "outputId": "3ce90629-2c20-4b8a-f022-b4dea2b77bbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:(10805, 8)\n",
            "valid:(1201, 8)\n",
            "test:(1334, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 51. 特徴量抽出  \n",
        "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ．   \n",
        "なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．  "
      ],
      "metadata": {
        "id": "dKQsynBjl48L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理（大文字をすべて小文字にする、数字を削除）\n",
        "def preprocessing(text):\n",
        "  text = \"\".join([i for i in text if i not in string.punctuation])\n",
        "  text = text.lower()\n",
        "  text = re.sub(\"[0-9]+\", \"\", text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "Ppa7WHmno-XO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データを特徴量に変換\n",
        "## TITLE を小文字にする\n",
        "data['TITLE'] = data['TITLE'].apply(lambda x: preprocessing(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFKtxolTmFfn",
        "outputId": "97919a42-3dde-4b5a-fd87-7c4897950ea8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-a49b5be817da>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['TITLE'] = data['TITLE'].apply(lambda x: preprocessing(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1-gram, 2-gramでTfidfを計算\n",
        "vectorizer = TfidfVectorizer(min_df=10, ngram_range=(1, 2))\n",
        "\n",
        "X = vectorizer.fit_transform(data['TITLE']).toarray()\n",
        "X = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "qIGVnqgTp0RP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train0, X_test = train_test_split(X, test_size=0.1, shuffle=True)\n",
        "X_train, X_val = train_test_split(X_train0, test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "vw20W1kTr4Sw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_csv(\"train.feature.txt\", sep=\"\\t\", index=False, header=None)\n",
        "X_val.to_csv(\"valid.feature.txt\", sep=\"\\t\", index=False, header=None)\n",
        "X_test.to_csv(\"test.feature.txt\", sep=\"\\t\", index=False, header=None)"
      ],
      "metadata": {
        "id": "fZBob-GqsMm5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 52. 学習\n",
        "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
      ],
      "metadata": {
        "id": "jdoasGzetQIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = data.iloc[X_train.index]['CATEGORY']\n",
        "y_val = data.iloc[X_val.index]['CATEGORY']\n",
        "y_test = data.iloc[X_test.index]['CATEGORY']"
      ],
      "metadata": {
        "id": "lriwaZWwsbpF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ロジスティック回帰  \n",
        "- 予測値を 0 ~ 1 に変換  \n",
        "- 確率が高いカテゴリを選択  \n",
        "  \n",
        "$\\sigma(x) = \\frac{1}{1 + \\exp^{-x}}$"
      ],
      "metadata": {
        "id": "uHYekKHovMVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(random_state=64, max_iter=10000)\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-D00XqTZtaEY",
        "outputId": "e827d8a4-7128-475d-da3c-10be4c257db1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=10000, random_state=64)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, random_state=64)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, random_state=64)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 53. 予測  \n",
        "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．  "
      ],
      "metadata": {
        "id": "MQfW1qaExbXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(X_val)\n",
        "y_val_pred_proba = model.predict_proba(X_val)"
      ],
      "metadata": {
        "id": "HJkGLN1_xYkM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44i1GX3Wx5Sq",
        "outputId": "77675349-8348-491f-dce6-5ae9b0bf358a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['b', 'e', 'e', ..., 'e', 'b', 'e'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred_proba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9at6uSdNx7XU",
        "outputId": "96f23554-f233-46b4-90fb-601d9408e4ff"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.94423657, 0.02483694, 0.0114473 , 0.01947919],\n",
              "       [0.03840239, 0.89660762, 0.03912327, 0.02586672],\n",
              "       [0.05042273, 0.91644765, 0.01617553, 0.01695409],\n",
              "       ...,\n",
              "       [0.37501741, 0.46415084, 0.069691  , 0.09114075],\n",
              "       [0.89938979, 0.02617146, 0.01566621, 0.05877255],\n",
              "       [0.04743907, 0.90500125, 0.0175499 , 0.03000978]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}